---
title: "Blood Donations"
author: "Bridget Bertoni"
date: "April 12, 2018"
output: pdf_document
---

```{r, fig.width=7.8, fig.height=7.8}
# read in data
train_data=read.csv(file="/Users/bbertoni/Desktop/github/Blood_Donations/training_data.csv",header=T)
test_data=read.csv(file="/Users/bbertoni/Desktop/github/Blood_Donations/test_data.csv",header=T)

names(train_data)=c("ID","Last_donation","Num_donations","Volume","First_donation","Made_donation")
names(test_data)=c("ID","Last_donation","Num_donations","Volume","First_donation")

head(train_data)
head(test_data)

#train_data$Made_donation=as.factor(train_data$Made_donation)
#test_data$Made_donation=as.factor(test_data$Made_donation)

# split training data into a training set and a validation set
set.seed(333)
train=sample(1:nrow(train_data),0.7*nrow(train_data),replace=F)
val_data = train_data[-train,]
train_data = train_data[train,]

# check for missing or strange values
sum(is.na(train_data))
sum(is.na(val_data))# no missing values
sum(is.na(test_data)) # missing values

plot(train_data)
par(mfrow=c(2,2))
plot(as.factor(train_data$Made_donation),train_data$Last_donation)
plot(as.factor(train_data$Made_donation),train_data$Num_donations)
plot(as.factor(train_data$Made_donation),train_data$First_donation)
plot(as.factor(train_data$Made_donation),train_data$Last_donation*train_data$Last_donation)
plot(as.factor(train_data$Made_donation),train_data$Num_donation*train_data$Num_donation)
plot(as.factor(train_data$Made_donation),train_data$First_donation*train_data$First_donation)
par(mfrow=c(1,1))

cor(train_data) # correlation between volume and the number of donations is 1

# fit basic logistic regression
glm.fit=glm(Made_donation~Last_donation+Num_donations+First_donation,data=train_data,
            family=binomial)
summary(glm.fit)
#plot(glm.fit)
glm.probs=predict(glm.fit,val_data,type="response")
glm.pred=rep(0,nrow(val_data))
glm.pred[glm.probs>0.5]=1
table(glm.pred,val_data$Made_donation) # confusion matrix
mean(glm.pred==val_data$Made_donation)
# log loss
-(1/nrow(val_data))*( sum(val_data$Made_donation*log(glm.probs)) + 
    sum((1-val_data$Made_donation)*log(1-glm.probs)) )

# calculate predictions using all of the training data
final_train_data=rbind(train_data,val_data)
glm.fit=glm(Made_donation~Last_donation+Num_donations+First_donation,data=train_data,
            family=binomial)
glm.probs=predict(glm.fit,test_data,type="response")
out=data.frame(test_data$ID,glm.probs)
names(out)=c("","Made Donation in March 2007")
write.csv(out,file="logreg_2018_04_16.csv",row.names=FALSE)

# fit basic logistic regression, drop first donation
glm.fit=glm(Made_donation~Last_donation+Num_donations,data=train_data,
            family=binomial)
summary(glm.fit)
#plot(glm.fit)
glm.probs=predict(glm.fit,val_data,type="response")
glm.pred=rep(0,nrow(val_data))
glm.pred[glm.probs>0.5]=1
table(glm.pred,val_data$Made_donation) # confusion matrix
mean(glm.pred==val_data$Made_donation)
# log loss
-(1/nrow(val_data))*( sum(val_data$Made_donation*log(glm.probs)) + 
    sum((1-val_data$Made_donation)*log(1-glm.probs)) )

# calculate predictions using all of the training data
final_train_data=rbind(train_data,val_data)
glm.fit=glm(Made_donation~Last_donation+Num_donations,data=train_data,
            family=binomial)
glm.probs=predict(glm.fit,test_data,type="response")
out=data.frame(test_data$ID,glm.probs)
names(out)=c("","Made Donation in March 2007")
write.csv(out,file="logreg_nofirstdon_2018_04_16.csv",row.names=FALSE)

# fit logistic regression with interaction terms
glm.fit=glm(Made_donation~Last_donation+Num_donations+First_donation+Last_donation:Num_donations,data=train_data,
            family=binomial)
summary(glm.fit)
#plot(glm.fit)
glm.probs=predict(glm.fit,val_data,type="response")
glm.pred=rep(0,nrow(val_data))
glm.pred[glm.probs>0.5]=1
table(glm.pred,val_data$Made_donation) # confusion matrix
mean(glm.pred==val_data$Made_donation)
# log loss
-(1/nrow(val_data))*( sum(val_data$Made_donation*log(glm.probs)) + 
    sum((1-val_data$Made_donation)*log(1-glm.probs)) )

# calculate predictions using all of the training data
final_train_data=rbind(train_data,val_data)
glm.fit=glm(Made_donation~Last_donation+Num_donations+First_donation+Last_donation:Num_donations,data=train_data,
            family=binomial)
glm.probs=predict(glm.fit,test_data,type="response")
out=data.frame(test_data$ID,glm.probs)
names(out)=c("","Made Donation in March 2007")
write.csv(out,file="logreg_int_2018_04_16.csv",row.names=FALSE)
```

```{r, fig.width=6.5, fig.height=4}
# fit KNN, choose k to minimize the cost on the validation set
library(class)
train.X=as.matrix(train_data[,-c(1,4,6)],nrow=nrow(train_data),ncol=ncol(train_data)-3)
val.X=as.matrix(val_data[,-c(1,4,6)],nrow=nrow(val_data),ncol=ncol(val_data)-3)
set.seed(111)
kvals=seq(1,nrow(train_data))
logloss=rep(NA,length(kvals))
for (k in 1:length(kvals)){
  knn.pred=knn(train.X,val.X,train_data$Made_donation,k=k,prob=TRUE)
  knn.prob=abs(attr(knn.pred,"prob")-10^-10) # need to add a fudge factor to deal
                                             # with logs of 0 and 1
  logloss[k]=-(1/nrow(val_data))*( sum(val_data$Made_donation*log(knn.prob)) + 
    sum((1-val_data$Made_donation)*log(1-knn.prob)) )
}
which.min(logloss)
logloss[which.min(logloss)]
plot(1:length(kvals),logloss)

# choose k at the elbow, note small k is high variance, low bias:
plot(1:50,logloss[1:50]) 

# calculate predictions using all of the training data
k=27 # pick k = 27
final_train.X=rbind(train.X,val.X)
test.X=as.matrix(test_data[,-c(1,4)],nrow=nrow(test_data),ncol=ncol(test_data)-2)
knn.pred=knn(final_train.X,test.X,c(train_data$Made_donation,val_data$Made_donation),k=k,
             prob=TRUE)
knn.prob=abs(attr(knn.pred,"prob")-10^-10)
out=data.frame(test_data$ID,knn.prob)
names(out)=c("","Made Donation in March 2007")
write.csv(out,file="knn_2018_04_16.csv",row.names=FALSE)

# fit a random forest with bagging
library(randomForest)
set.seed(200)
train_data$Made_donation=as.factor(train_data$Made_donation)
val_data$Made_donation=as.factor(val_data$Made_donation)
#test_data$Made_donation=as.factor(test_data$Made_donation)
m=ncol(train_data)-3
bag.data=randomForest(Made_donation~Last_donation+Num_donations+First_donation+Last_donation:Num_donations,data=train_data[,-c(1,4)],mtry=m,importance=TRUE)
bag.data
importance(bag.data)
probs=predict(bag.data,newdata=val_data,type="prob")[,2]+10^-10
# log loss
-(1/nrow(val_data))*( sum((as.numeric(val_data$Made_donation)-1)*log(probs)) + 
    sum((1-(as.numeric(val_data$Made_donation)-1))*log(1-probs)) )

# fit a random forest with boosting
library(gbm)
lambdas=c(10^-5,10^-4,10^-3,10^-2,0.05,0.1,0.2,0.5)
logloss=rep(NA,length(lambdas))
train_data_exp=cbind(train_data,train_data$Last_donation*train_data$Num_donations)
names(train_data_exp)[7]="Last_num_int"
val_data_exp=cbind(val_data,val_data$Last_donation*val_data$Num_donations)
names(val_data_exp)[7]="Last_num_int"
for (i in 1:length(lambdas)){
  lambda=lambdas[i]
  boost.data=gbm(Made_donation~Last_donation+Num_donations+First_donation+Last_num_int,data=train_data_exp,
                 distribution="bernoulli",n.trees=1000,shrinkage=lambda,interaction.depth=2)
  probs=predict(bag.data,newdata=val_data_exp,type="prob")[,2]+10^-10
  probs
  logloss[i]=-(1/nrow(val_data))*( sum((as.numeric(val_data$Made_donation)-1)*log(probs)) + 
    sum((1-(as.numeric(val_data$Made_donation)-1))*log(1-probs)) )
}
logloss
```

